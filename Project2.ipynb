{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Packages and dataset load**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "# import dataframe_image as df___i\n",
    "\n",
    "color = {\"granate\":\"#BA4A00\",\n",
    "         \"amarillo\":\"#F5B041\",\n",
    "         \"verde\":\"#148F77\",\n",
    "         \"blue\":\"#0051A2\",\n",
    "         \"red\": \"#DD1717\"}\n",
    "color_palette = [color[\"blue\"], 'darkorchid', color['verde'], color['amarillo'],'gray', 'cornflowerblue', color['red']]\n",
    "sb.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Dry_Bean_Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Auxiliar functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(models_names: list, columns_names: list, k1: int):\n",
    "    header = pd.MultiIndex.from_product([models_names, columns_names])\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    df['KFold'] = np.arange(1, k1+1)\n",
    "    df.set_index('KFold', inplace=True)\n",
    "    return df\n",
    "\n",
    "def twolevelcv(X: np.array, y: np.array, k1: int, k2: int, models: list, params: dict, rs: int):\n",
    "    \"\"\"Allows to compute two level crossvalidation.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Features (numeric)\n",
    "        y (np.array): Class (objective variable)\n",
    "        k1 (int): Nº of outer folds\n",
    "        k2 (int): Nº of inner folds\n",
    "        models (list): List of models for comparison\n",
    "        params (dict): Dictionary including the set of parameters. In this case we only tune 1 parameter per model.\n",
    "        rs (int): Random state\n",
    "    Returns:\n",
    "        df: Dataframe\n",
    "    \"\"\"\n",
    "    test_error_dict = {}\n",
    "    k = 0\n",
    "    names = [type(m).__name__ for m in models]\n",
    "    col_names = ['Param. Value', 'Error']\n",
    "    df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = StratifiedKFold(k1, shuffle = True, random_state=rs)\n",
    "    # first level split\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k += 1\n",
    "        kf2 = StratifiedKFold(k2, shuffle = True, random_state=rs)\n",
    "        print(f'Computing KFold {k}/{k1}...')\n",
    "        # second level split\n",
    "        for train_idx2, test_idx2 in tqdm(kf2.split(X[train_idx1, :], y[train_idx1]), total = k2):\n",
    "            X_train = X[train_idx2, :]\n",
    "            y_train = y[train_idx2]\n",
    "            X_test = X[test_idx2, :]\n",
    "            y_test = y[test_idx2]\n",
    "            for name, model in zip(names, models):\n",
    "                if name != 'DummyClassifier':\n",
    "                    pname = list(params[name].keys())[0]\n",
    "                    error_test = []\n",
    "                    for p_ in params[name][pname]:\n",
    "                        pdict = {pname: p_}\n",
    "                        model = model.set_params(**pdict)\n",
    "                        # train the model\n",
    "                        model.fit(X_train, y_train)\n",
    "                        # evaluate performance\n",
    "                        pred2_test = model.predict(X_test)\n",
    "                        error_test.append(np.sum(pred2_test != y_test)/ y_test.shape[0])\n",
    "                    min_param = params[name][pname][np.argmin(error_test)]\n",
    "                else:\n",
    "                    model.fit(X_train, y_train)\n",
    "                    pred2_test = model.predict(X_test)\n",
    "                    error_test = np.sum(pred2_test != y_test)/ y_test.shape[0]\n",
    "                    min_param = np.NaN\n",
    "                df.loc(axis = 1)[name, 'Error'][k] = np.min(error_test)\n",
    "                df.loc(axis = 1)[name, 'Param. Value'][k] = min_param\n",
    "    return df, test_idx1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad43814",
   "metadata": {},
   "source": [
    "# **1 - Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "· NUMBER OF FEATURES: 16\n",
      "\n",
      "· FEATURES:\n",
      " ['Area' 'Perimeter' 'MajorAxisLength' 'MinorAxisLength' 'AspectRation'\n",
      " 'Eccentricity' 'ConvexArea' 'EquivDiameter' 'Extent' 'Solidity'\n",
      " 'roundness' 'Compactness' 'ShapeFactor1' 'ShapeFactor2' 'ShapeFactor3'\n",
      " 'ShapeFactor4']\n",
      "\n",
      "· NUMBER OF DATA POINTS: 13611\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns.values\n",
    "X = df.drop(columns='Class').values\n",
    "y = df['roundness']\n",
    "\n",
    "\n",
    "print('· NUMBER OF FEATURES:', X.shape[1])\n",
    "print('\\n· FEATURES:\\n', columns[:-1])\n",
    "print('\\n· NUMBER OF DATA POINTS:', X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part A. *Linear regression.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part B. *Other models. Evaluation.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate training data x and label y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13611, 16)\n"
     ]
    }
   ],
   "source": [
    "label_feature = 'roundness'\n",
    "\n",
    "x = df.drop(columns='Class').drop(columns=label_feature).values\n",
    "Class = df['Class']\n",
    "class_label = []\n",
    "dictionary = {\n",
    "    'SEKER': 0,\n",
    "    'BARBUNYA': 1,\n",
    "    'BOMBAY': 2, \n",
    "    'CALI': 3,\n",
    "    'HOROZ': 4,\n",
    "    'SIRA': 5,\n",
    "    'DERMASON': 6\n",
    "}\n",
    "for i in Class:\n",
    "    class_label.append(dictionary[i])\n",
    "\n",
    "class_label = np.array(class_label)\n",
    "x = np.array(x)\n",
    "x = np.insert(x, x.shape[1], class_label, axis=1)\n",
    "print(x.shape)\n",
    "y = df[label_feature]\n",
    "y = np.array(y)\n",
    "y = y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3985587e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:212: LinAlgWarning: Ill-conditioned matrix (rcond=1.166e-19): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T\n"
     ]
    }
   ],
   "source": [
    "import toolbox_02450\n",
    "from matplotlib.pyplot import figure, plot, xlabel, ylabel, legend, show\n",
    "import sklearn.linear_model as lm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "X = x\n",
    "# Fit ordinary least squares regression model\n",
    "# model = lm.LinearRegression(fit_intercept=True)\n",
    "model = lm.Ridge(alpha = 0.0, fit_intercept=True)\n",
    "model = model.fit(X,y)\n",
    "# Compute model output:\n",
    "y_est = model.predict(X)\n",
    "loss = nn.MSELoss()(torch.from_numpy(y).to(device), torch.from_numpy(y_est).to(device))\n",
    "print(loss.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.393893878674301e-05\n"
     ]
    }
   ],
   "source": [
    "class Linear_Regression():\n",
    "    def __init__(self, weight = 0.0) :\n",
    "        self.weight = weight\n",
    "        self.w = None\n",
    "    \n",
    "    def fit(self,  X: np.array, y: np.array):\n",
    "        x0 = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X,x0),axis=1)\n",
    "        X = np.matrix(X)\n",
    "        y = np.matrix(y)\n",
    "        w = np.matmul(X.T, X)\n",
    "        w = w + self.weight * np.eye(X.shape[1], X.shape[1])\n",
    "        w = w.I * (X.T * y)\n",
    "        self.w = w\n",
    "        return w\n",
    "    \n",
    "    def predict(self, X: np.array):\n",
    "        x0 = np.ones((X.shape[0], 1))\n",
    "        X = np.concatenate((X,x0),axis=1)\n",
    "        X = np.matrix(X)\n",
    "        y_pred = X * self.w\n",
    "        return y_pred\n",
    "\n",
    "model = Linear_Regression(0.0)\n",
    "w = model.fit(X, y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "loss = nn.MSELoss()(torch.from_numpy(y).to(device), torch.from_numpy(y_pred).to(device))\n",
    "print(loss.cpu().detach().numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "13611\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.pylab import figure, plot, xlabel, ylabel, legend, ylim, show\n",
    "import sklearn.linear_model as lm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device: \", device)\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, num_input, num_output, num_hidden):\n",
    "        super(ANN, self).__init__()\n",
    "        self.Net = nn.Sequential(\n",
    "            nn.Linear(num_input, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_output),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return self.Net(input)\n",
    "\n",
    "model = ANN(x.shape[1], y.shape[1], 64).to(device)\n",
    "\n",
    "batch_size = 512\n",
    "x = x.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "x_train = torch.from_numpy(x).to(device)\n",
    "y_train = torch.from_numpy(y).to(device)\n",
    "print(x_train.shape[0])\n",
    "train_set = TensorDataset(x_train, y_train)\n",
    "train_set = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 1000\n",
    "loss_func = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing KFold 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [10:59<00:00, 66.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing KFold 2/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [11:52<00:00, 71.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing KFold 3/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [03:55<09:09, 78.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 86\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[39m# for name, model in zip(names, models):\u001b[39;00m\n\u001b[0;32m     65\u001b[0m             \u001b[39m#     if name != 'DummyClassifier':\u001b[39;00m\n\u001b[0;32m     66\u001b[0m             \u001b[39m#         pname = list(params[name].keys())[0]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[39m#     df.loc(axis = 1)[name, 'Error'][k] = np.min(error_test)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m             \u001b[39m#     df.loc(axis = 1)[name, 'Param. Value'][k] = min_param\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(generalization_error)\n\u001b[1;32m---> 86\u001b[0m error \u001b[39m=\u001b[39m twolevelcv_reg(x, y, \u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[0;32m     87\u001b[0m \u001b[39mprint\u001b[39m(error)\n",
      "Cell \u001b[1;32mIn [24], line 51\u001b[0m, in \u001b[0;36mtwolevelcv_reg\u001b[1;34m(X, y, k1, k2, rs)\u001b[0m\n\u001b[0;32m     49\u001b[0m optim \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200\u001b[39m):\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mfor\u001b[39;00m x_, y_ \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     52\u001b[0m     \u001b[39m#     lr = 3e-4 * (1.0 - float(i) / float(epochs))\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[39m#     optim = torch.optim.Adam(model.parameters(), lr=lr)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m         output \u001b[39m=\u001b[39m model(x_)\n\u001b[0;32m     55\u001b[0m         loss \u001b[39m=\u001b[39m loss_func(output, y_)\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\utils\\data\\dataset.py:193\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39;49m(tensor[index] \u001b[39mfor\u001b[39;49;00m tensor \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensors)\n",
      "File \u001b[1;32md:\\Users\\karl\\anaconda3\\envs\\pytorch2\\lib\\site-packages\\torch\\utils\\data\\dataset.py:193\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m--> 193\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m(tensor[index] \u001b[39mfor\u001b[39;00m tensor \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtensors)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def twolevelcv_reg(X: np.array, y: np.array, k1: int, k2: int, rs: int):\n",
    "    \"\"\"Allows to compute two level crossvalidation.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Features (numeric)\n",
    "        y (np.array): Class (objective variable)\n",
    "        k1 (int): Nº of outer folds\n",
    "        k2 (int): Nº of inner folds\n",
    "        models (list): List of models for comparison\n",
    "        params (dict): Dictionary including the set of parameters. In this case we only tune 1 parameter per model.\n",
    "        rs (int): Random state\n",
    "    Returns:\n",
    "        df: Dataframe\n",
    "    \"\"\"\n",
    "    test_error_dict = {}\n",
    "    k = 0\n",
    "    # names = [type(m).__name__ for m in models]\n",
    "    col_names = ['Param. Value', 'Error']\n",
    "    # df = dataset_creator(names, col_names, k1)\n",
    "    kf1 = KFold(n_splits=k1, shuffle = True, random_state=rs)\n",
    "    # first level split\n",
    "    for train_idx1, test_idx1 in kf1.split(X, y):\n",
    "        k += 1\n",
    "        kf2 = KFold(n_splits=k2, shuffle = True, random_state=rs)\n",
    "        print(f'Computing KFold {k}/{k1}...')\n",
    "        # second level split\n",
    "        for train_idx2, test_idx2 in tqdm(kf2.split(X[train_idx1, :], y[train_idx1]), total = k2):\n",
    "            X_train = X[train_idx2, :]\n",
    "            y_train = y[train_idx2]\n",
    "            X_test = X[test_idx2, :]\n",
    "            y_test = y[test_idx2]\n",
    "            generalization_x_test = torch.from_numpy(X[test_idx1, :]).to(device)\n",
    "            generalization_y_test = torch.from_numpy(y[test_idx1]).to(device)\n",
    "            generalization_error = []\n",
    "            error_test = []\n",
    "            x_train_ = torch.from_numpy(X_train).to(device)\n",
    "            y_train_ = torch.from_numpy(y_train).to(device)\n",
    "            X_test_ = torch.from_numpy(X_test).to(device)\n",
    "            y_test_ = torch.from_numpy(y_test).to(device)\n",
    "            train_loader = TensorDataset(x_train_, y_train_)\n",
    "            train_loader = DataLoader(train_loader, batch_size=x_train_.shape[0], shuffle=True)\n",
    "            for i in range(3):\n",
    "                # test_loader = TensorDataset(X_test, y_test)\n",
    "                # test_loader = DataLoader(test_loader, batch_size=X_test.shape[0], shuffle=True)\n",
    "                model = ANN(16, 1, 2 ** i).to(device)\n",
    "                model.train()\n",
    "                loss_func = torch.nn.MSELoss()\n",
    "                optim = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "                for i in range(200):\n",
    "                    for x_, y_ in train_loader:\n",
    "                    #     lr = 3e-4 * (1.0 - float(i) / float(epochs))\n",
    "                    #     optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                        output = model(x_)\n",
    "                        loss = loss_func(output, y_)\n",
    "                        optim.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optim.step()\n",
    "                model.eval()\n",
    "                output = model(generalization_x_test)\n",
    "                loss = loss_func(output, generalization_y_test)\n",
    "                generalization_error.append(loss.cpu().detach().numpy())\n",
    "            \n",
    "            # for name, model in zip(names, models):\n",
    "            #     if name != 'DummyClassifier':\n",
    "            #         pname = list(params[name].keys())[0]\n",
    "            #         error_test = []\n",
    "            #         for p_ in params[name][pname]:\n",
    "            #             pdict = {pname: p_}\n",
    "            #             model = model.set_params(**pdict)\n",
    "            #             # train the model\n",
    "            #             model.fit(X_train, y_train)\n",
    "            #             # evaluate performance\n",
    "            #             pred2_test = model.predict(X_test)\n",
    "            #             error_test.append(np.sum(pred2_test != y_test)/ y_test.shape[0])\n",
    "            #         min_param = params[name][pname][np.argmin(error_test)]\n",
    "            #     else:\n",
    "            #         model.fit(X_train, y_train)\n",
    "            #         pred2_test = model.predict(X_test)\n",
    "            #         error_test = np.sum(pred2_test != y_test)/ y_test.shape[0]\n",
    "            #         min_param = np.NaN\n",
    "            #     df.loc(axis = 1)[name, 'Error'][k] = np.min(error_test)\n",
    "            #     df.loc(axis = 1)[name, 'Param. Value'][k] = min_param\n",
    "    return np.array(generalization_error)\n",
    "\n",
    "error = twolevelcv_reg(x, y, 10, 10, 1)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad43814",
   "metadata": {},
   "source": [
    "# **2 - Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying object without editing the original\n",
    "df_ = df.copy(deep=True)\n",
    "# Doing this we can choose to use outliers filter or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df_.columns.values\n",
    "X = df_.drop(columns='Class').values\n",
    "y = df_['Class']\n",
    "le = LabelEncoder()\n",
    "y_ = le.fit_transform(y)\n",
    "classes = y.unique()\n",
    "\n",
    "print('· NUMBER OF FEATURES:', X.shape[1])\n",
    "print('\\n· FEATURES:', columns[:-1])\n",
    "print('\\n· NUMBER OF DATA POINTS:', X.shape[0])\n",
    "print('\\n· CLASSES:', classes)\n",
    "print('\\n· NUMBER OF CLASSES:', len(classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transformations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold_ = 3\n",
    "outlier_index = []\n",
    "df_ = pd.DataFrame(columns=df.columns)\n",
    "index = 0\n",
    "for K in classes:\n",
    "    outlier_index = []\n",
    "    a = df.loc[df[\"Class\"] == K]\n",
    "    value = a.drop(columns='Class').values\n",
    "    for j in range(16):\n",
    "        std = np.std(value[:, j])\n",
    "        mean = np.mean(value[:, j])\n",
    "        for i in range(value[:, j].shape[0]):\n",
    "            if (value[i, j] - mean) / std > Threshold_:\n",
    "                outlier_index.append(i + index)\n",
    "    index = i + index + 1\n",
    "    outlier_index = np.unique(outlier_index)\n",
    "    a = a.drop(outlier_index)\n",
    "    df_ = pd.concat([df_,a])\n",
    "df_.reset_index(drop=True, inplace=True)\n",
    "print(f'Filtered outliers: {df.shape[0] - df_.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standarization of the dataset\n",
    "sc = StandardScaler()\n",
    "X_stdz = sc.fit_transform(X)\n",
    "df_stdz = pd.DataFrame(columns = columns[:-1], data = X_stdz)\n",
    "df_stdz['Class'] = y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Logistic regression *vs.* Neural Network *vs.* Baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# # evaluate the model and collect the scores\n",
    "# n_scores = cross_val_score(model, X_stdz, y_, scoring = 'accuracy', cv=cv, n_jobs=-1)\n",
    "# # report the model performance\n",
    "# print('Mean Accuracy: %.3f (+-%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Cross-Validation table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "lam = np.logspace(-6, 2, 100)\n",
    "C = 1/ lam\n",
    "# C = [200000000, 10000000, 0.1519911082952933, 0.2848035868435805 ]\n",
    "params['LogisticRegression'] = {'C': C}\n",
    "params['DummyClassifier'] = [None]\n",
    "params['MLPClassifier'] = {'hidden_layer_sizes': [(8, ), (16, ), (20, )]}\n",
    "models = [LogisticRegression(multi_class='multinomial', solver='saga', max_iter=1000000, random_state= random_state, tol = 0.003, n_jobs = -1),\n",
    "        DummyClassifier(strategy='most_frequent', random_state=random_state),\n",
    "        MLPClassifier(solver='adam', activation='logistic', alpha=1e-4, random_state=random_state, max_iter=1000, \n",
    "        early_stopping=True, validation_fraction=0.2, warm_start=True, verbose=False, learning_rate ='adaptive', learning_rate_init=0.01)]\n",
    "k1 = 10\n",
    "k2 = 10\n",
    "Table, test_set_outer = twolevelcv(X = X_stdz, y = y_, k1 = k1, k2 = k2, models = models, params = params, rs = random_state)\n",
    "Table.to_csv('Results/Test2_saga.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Table = Table.round(3)\n",
    "Table.to_csv(r'Results\\Table_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_outer.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Stadistical Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def McNemar(models: list, X: np.array, y: np.array, k1: int, rs: int):\n",
    "    \"\"\"Computes the McNemar matrix for all the models in the list \n",
    "\n",
    "    Args:\n",
    "        models (list): list of models\n",
    "        X (np.array): Features\n",
    "        y (np.array): Classes\n",
    "        k1 (int): Number of folds\n",
    "        rs (int): Random state\n",
    "\n",
    "    Returns:\n",
    "        _type_: Matrix as a dictionary\n",
    "    \"\"\"\n",
    "    kf1 = StratifiedKFold(k1, shuffle = True, random_state=rs)\n",
    "    k = 0\n",
    "    # setting up all the possible combinations between the different models\n",
    "    matrix = dict.fromkeys(combinations(range(len(models)), 2))\n",
    "    matrix_tot = dict.fromkeys(combinations(range(len(models)), 2))\n",
    "    for train_idx, test_idx in kf1.split(X, y):\n",
    "        test_size = test_idx.shape[0]\n",
    "        yABC = np.empty(shape=(len(models), test_size))\n",
    "        for i, model in enumerate(models):\n",
    "            model.fit(X[train_idx,:], y[train_idx])\n",
    "            y_pred = model.predict(X[test_idx, :])\n",
    "            yABC[i, :] = 1*(y_pred == y[test_idx])\n",
    "        for j in list(matrix.keys()):\n",
    "            if k == 0:\n",
    "                matrix[j] = np.empty(shape=(k1, 4))\n",
    "            n11 = np.sum(yABC[j[0],:]*yABC[j[1],:])\n",
    "            n12 = np.sum(yABC[j[0],:]*(1-yABC[j[1],:]))\n",
    "            n21 = np.sum(yABC[j[1],:]*(1-yABC[j[0],:]))\n",
    "            n22 = np.sum((1-yABC[0,:])*(1-yABC[1,:]))\n",
    "            matrix[j][k] = np.array([n11, n12, n21, n22])\n",
    "            matrix_tot[j] = matrix[j].sum(axis = 0)\n",
    "        k+=1\n",
    "    return matrix, matrix_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 1\n",
    "models = [LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state= random_state, C = 0.07),\n",
    "        DummyClassifier(strategy='most_frequent', random_state=random_state),\n",
    "        MLPClassifier(solver='adam', activation='logistic', alpha=1e-4, random_state=random_state, max_iter=1000, hidden_layer_sizes=(8, ),\n",
    "        early_stopping=True, validation_fraction=0.2, warm_start=True, verbose=False, learning_rate ='adaptive', learning_rate_init=0.01)]\n",
    "k1 = 10\n",
    "mfull, m_tot = McNemar(models, X_stdz[test_set_outer, :], y_[test_set_outer], k1, rs = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mat in mfull:\n",
    "    print('          ',mat)\n",
    "    for ma in mfull[mat]:\n",
    "        print('      ', ma)\n",
    "    print('\\nTotal:',m_tot[mat])\n",
    "    print('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the matrix with shape (3: number of models, 10: nº of k-folds, 4: matrix shape)\n",
    "\n",
    "The matrix is squeezed so we have:\n",
    "$$\\begin{bmatrix}\n",
    " n_{11}    & n_{12}  \\\\\n",
    " n_{21}    & n_{22}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Here is $[ n_{11}, n_{12}, n_{21}, n_{22} ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H_0:$ Model A has the same performarnce as model B \n",
    "\n",
    "$H_1:$ Model A and model B has different performance\n",
    "\n",
    "small p_value-> we discard H0 -> Model A and Model B have different performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we set:\n",
    "\n",
    "**Model 0**: LR\n",
    "\n",
    "**Model 1**: Baseline\n",
    "\n",
    "**Model 2**: MLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***P-Values***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combs = list(combinations(range(len(models)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "pv_dict = dict.fromkeys(combs)\n",
    "for j in combs:\n",
    "    vals = m[j][:, 1:3]\n",
    "    pv_dict[j] = [binom.cdf(min(vals[i]), n = sum(vals[i]), p = 1/2) for i in range(len(vals))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "McNemar_pv = pd.DataFrame(columns = combs, index = range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for n in pv_dict.values():\n",
    "    print(f'{combs[i]}')\n",
    "    for j, k in enumerate(n):\n",
    "        McNemar_pv[combs[i]][j] = \"{:.3e}\".format(k)\n",
    "        text = str(\"${:.2e}\".format(k))\n",
    "        text = text + '}$'\n",
    "        text = text.replace(\"e\", \"·10^{\")\n",
    "        print(text)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "McNemar_pv.to_csv('Results/McNemar_pv_best.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcs(mat: np.array):\n",
    "    \"\"\"Calculate f y g from a McNemar Matrix\n",
    "    Args:\n",
    "        matrix (np.array): McNemar matrix from one K-fold\n",
    "    Returns:\n",
    "        _type_: f and g\n",
    "    \"\"\"\n",
    "    n = mat.sum()\n",
    "    n12 = mat[1]\n",
    "    n21 = mat[2]\n",
    "    E_th = (n12 - n21)/n \n",
    "    Q = (n**2 * (n+1) * (E_th +1) * (1-E_th)) / (n * (n12 + n21) - (n12 - n21)**2)\n",
    "    f = (Q-1)*(E_th+1)/2\n",
    "    g = (Q-1)*(1-E_th)/2\n",
    "    return f, g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interval(mat: np.array, alpha: float):\n",
    "    \"\"\"McNemar confidence interval\n",
    "\n",
    "    Args:\n",
    "        alpha (float): The desired confidence (should be 0.05)  \n",
    "        f (_type_): output of calcs\n",
    "        g (_type_): output of calcs \n",
    "\n",
    "    Returns:\n",
    "        _type_: left and right bounds from the interval\n",
    "    \"\"\"\n",
    "    f,g = calcs(mat)\n",
    "    theta_L = 2*beta.ppf(alpha, f, g) - 1\n",
    "    theta_R = 2*beta.ppf(1 - alpha/2, f, g) - 1\n",
    "    return theta_L, theta_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in m:\n",
    "    print(i)\n",
    "    for mat in m[i]:\n",
    "        theta_L, theta_R = interval(mat, 0.05)\n",
    "        print(f'[{np.round(theta_L, 2)}, {np.round(theta_R, 2)}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.5 Train logistic regression model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the fourth exercise do we have to repeat the parameter selection process or can just go ahead with the best parameter selection for each model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3051d3c5513a3f8d66cffc6293c1d26977f11d89a064d2eaf959b056923e92ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
